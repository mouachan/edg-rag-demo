apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-3-14b-instruct
  namespace: edg-demo
  annotations:
    opendatahub.io/genai-use-case: chat rag
    opendatahub.io/hardware-profile-name: nvidia-gpu-l40
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/model-type: generative
    openshift.io/description: ""
    openshift.io/display-name: mistral-3-14b-instruct
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/stop: "false"
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: "true"
    opendatahub.io/genai-asset: "true"
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      resources:
        limits:
          cpu: "8"
          memory: 16Gi
          nvidia.com/gpu: "2"
        requests:
          cpu: "8"
          memory: 16Gi
          nvidia.com/gpu: "2"
      runtime: mistral-3-14b-instruct
      storageUri: pvc://mistral-model-storage
    nodeSelector:
      topology.kubernetes.io/zone: eu-central-1a
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Equal
      value: NVIDIA-L40-PRIVATE
